---
title: "MovieLens Report"
author: "Masayoshi Sato"
date: "2021/5/3"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Introduction

Nowadays, recommendation systems have become common to us. Many products and services are rated by consumers using stars and points, and these ratings are utilized by companies and organizations. They are precious; they are not only helpful for customers to choose goods or services, but are also valuable for makers and service providers in terms of predicting consumers behavior. If you explore user ratings in detail, you will find their customers preference, and be able to recommend things which are most likely to be bought or used.

One of the field in which this system is used is movie industries. In this paper, I will take a movie recommendation dataset, explore its content, and build a model which will predict ratings as accurately as possible.

### 1.Dataset

The data used in this report, MovieLens 10M Dataset, is available at the Grouplens website. According to the site, this data set has 10000054 ratings and 95580 tags applied to 10681 movies by 71567 users of the online movie recommender service MovieLens.

MovieLens 10M dataset: <https://grouplens.org/datasets/movielens/10m/>

### 2.Goal

The goal is to train a machine learning algorithm to predict movie ratings based on factors in the provided dataset.

According to the GroupLens web site, it has several factors, such as user ID, movieID, title, genres, rating as well. The assumption in this paper is that they are inter-correlated and helpful to forecast ratings. The challenge of a recommendation system, however, is that the data is sparse and each factor in the dataset might affect others. Namely, some users rate movies far more than others, and some movies are rated more. Contrarily, some movies are rated by very small number of users. Moreover, genres, released year may also affect ratings.

Therefore, a linear regression will be used to find weights that minimize the residual mean squared error, the RMSE. Predictors are user ID, movie ID, genres. and released year. In addition to this, regulation is used to penalize large estimates that are produced by using small samples. The result of model finding is assessed by its value of the RMSE (root mean squared error).

## Exploratory Analysis

The downloaded data is split into two, the edx dataset (90%) and the validation dataset (10%).

```{r include=FALSE}
library(tidyverse)
library(caret)
library(data.table)
library(lubridate)

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),title = as.character(title),genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

The validation dataset has 999999 rows and 6 columns. And the edx dataset has 9000055 rows and 6 columns. The validation dataset is left to final evaluation by a model explored in this paper.

```{r eval=FALSE, include=FALSE}
dim(edx)
dim(validation)
```

The first six rows of the edx dataset is as follows;

```{r echo=FALSE}
head(edx)
```

UserId indicates a person who rate movies. Users rate multiple movies. Each movie corresponds to movieId and title. Title has a movie's release year in a parenthesis. Movies are categorized by genres, which consists of several words, such as "Comedy", "Crime", or "Sci-Fi". They are separated by "\|".

We will investigate columns to find out their features.

### 1.Rating

Ratings ranges from 0.5 to 5.0 by 0.5. 5.0 is the highest. The distribution is right-skewed. The mean of the rating is 3.51. The median is 4.

```{r echo=FALSE}
edx %>% ggplot(aes(rating)) + geom_bar() + ggtitle("Ratings")
```

```{r eval=FALSE, include=FALSE}
unique(edx$rating)%>% sort()
mean(edx$rating)
median(edx$rating)
```

### 2.Users

The number of user (unique user) is 69878. As is seen in the first six rows of the edx, some users rate multiple movies. It's distribution is left-skewed. Some are very active in rating movies (more than 1,000).

```{r echo=FALSE}
edx %>%
  dplyr::count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() +
  ggtitle("Users")

```

```{r eval=FALSE, include=FALSE}
n_distinct(edx$userId)
```

### 3.Movies

10677 movies (the number of unique movies) are rated. They are rated by multiple users. The histogram tells us some movies get very small amount of ratings (less than 10) and some get huge amount of ratings (more than 10,000).

```{r echo=FALSE}
edx %>% dplyr::count(movieId) %>% 
  ggplot(aes(n)) + geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + ggtitle("Movies")
```

```{r eval=FALSE, include=FALSE}
n_distinct(edx$movieId)
```

### 4.Timestamp

The class of "timestamp" is an integer. The numbers are produced by counting days from the day, "1970.01.01". To make them more understandable, we will covert them to POSIXct data at first.

```{r include=FALSE}
edx$timestamp <- as.POSIXct(edx$timestamp, origin= "1970-01-01",tz="GMT") 
```

Then we mutate a new column "date". The column have a month and year of the date when a movie being rated.

```{r include=FALSE}
edx <- edx %>% mutate(date = round_date(timestamp, unit = "month"))
```

We plot rating and date with a regression line. As is seen from the graph, points are mainly gathered around 3.5 rating, and the regression line is relatively flat.

```{r echo=FALSE}
edx %>% group_by(date) %>% 
  summarize(ratings =mean(rating)) %>%
  ggplot(aes(x = date, y=ratings)) +
  geom_point() + geom_smooth(method = "lm")+ ggtitle("Months of rating")
```

### 5.Title

Titles are difficult to deal with as they are. But they have release years in parentheses. We extract years from titles. Then we mutate new columns, "release year". We plot rating and release year with a regression line. There seems to be a tendency that recent movies are rated low and old movies are rated high. The release year works as effects to predict rating.

```{r include=FALSE}
edx <- edx %>% 
  mutate(release_year = as.numeric(str_sub(title,-5,-2))) 
```

```{r echo=FALSE}
edx %>% group_by(release_year) %>% 
  summarize(ratings = mean(rating)) %>% 
  ggplot(aes(release_year, ratings)) + geom_point()+geom_smooth(method="lm")+      
  ggtitle(" Release_year")
```

### 6.Genres

Some like comedy, but do not like thriller. Some like Sci-fi but do not like animation. Such a preference naturally affects movies' rating.

```{r eval=FALSE, include=FALSE}
edx$genres %>% n_distinct()
```

The edx has unique 797 genres. Here are top 10 genres in terms of high rating.

```{r echo=FALSE}
edx %>% group_by(genres)%>% summarize(ratings = mean(rating))%>% 
  top_n(10,ratings) %>% arrange(desc(ratings))
```

Each genre consists of several words, such as "Action", "Drama", or"Crime". Instead of splitting them, we try to deal with them as one combination, and find correlation between rating and genre. To make it simple, genres which have more than 20000 ratings are plotted in the graph.

As in seen in the graph, some genres are rated very low (minimum approx. 2.8) and some are rated very high (maximum approx. 4.2). From this, we understand that genres (even though they are combined) affect ratings considerably.

```{r echo=FALSE}
edx %>% group_by(genres) %>% 
  summarize(n = n(), avg = mean(rating)) %>% 
  filter(n >= 20000) %>% 
  mutate(genres = reorder(genres,avg)) %>% 
  ggplot(aes(x = genres, y = avg)) + geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ 
  ggtitle("genres (n>20000)")
```

### 7.Exploration Summary

From these studies, we found that "movieId", "userId", "genres", and "release year (extracted from "title") are important factors deciding rating. We leave out the rating dates since they seem not to have much significance. Using these factors, we will make a linear regression model in the next chapter.

In order to make models, we split the edx dataset into training set and test set.

```{r include=FALSE}
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
```

To make sure we don't include users and movies in the test set that do not appear in the training set, we remove these entries using the semi_join function.

```{r include=FALSE}
test_set <- test_set %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```

To assess models, we use RMSE. It is defined as : $$
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
$$

Here, $y_{u,i}$ is rating for movie $i$ by user *i.* $\hat{y}_{u,i}$ is prediction, and $N$is the number of user/ movie combinations. Models, which will be described in the next chapter, are evaluated by its RMSE. We create R function for this purpose.

```{r echo=TRUE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The model which minimize RMSE in the test set will be applied to the validation dataset, and extract its RMSE.

## Finding Models

### 1 Baseline model, assuming the same rating for all movies

First, we make a model assuming ratings of all movies are the same. We use the average of all movies ratings. Here is a code to produce its RMSE.

```{r echo=TRUE}
mu <- mean(train_set$rating)
naive_rmse <- RMSE(test_set$rating, mu)
naive_rmse
```

This is a baseline of our further modeling. We name it "Average rating".

```{r echo=FALSE}
rmse_results <- tibble(method = "1. Average Rating Model", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

### 2 Movie effects

Next, we create a linear regression model in which movie effects are taken into account. Here is an equation. $Y{u,i}$ is outcome. $\mu$ is the average of ratings. $b_i$ is movie effect. $\epsilon_{u,i}$ is independent error.

$$
Y_{u, i} = \mu + b_i + \epsilon_{u, i}
$$ This is a code to calculate the RMSE.

```{r echo=TRUE}
mu <- mean(train_set$rating)

movie_avg <- train_set %>%
  group_by(movieId)%>%
  summarize(b_i =mean(rating -mu))

movie_effect_pred <- mu + test_set %>%
  left_join(movie_avg, by="movieId") %>%
  pull(b_i)

movie_effect_rmse <- RMSE(test_set$rating, movie_effect_pred)
movie_effect_rmse
```

The RMSE is improved. We call it "Movie Effect Model".

```{r echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method="2. Movie Effects Model",
                                 RMSE =movie_effect_rmse))
rmse_results %>% knitr::kable()
```

### 3 Movie and user effects

As well as movie effect, user effect can affect the prediction. In this model, both movie and user effect are considered. This model can be represented in this equation. $b_u$ is user effect. ;

$$
Y_{u, i} = \mu + b_i + b_u + \epsilon_{u, i} 
$$

Here is a code to produce the RMSE.

```{r echo=TRUE}
user_avg <- train_set %>%
  left_join(movie_avg, by= "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating- mu -b_i))

movie_user_effect_pred <- test_set %>%
  left_join(movie_avg, by="movieId") %>%
  left_join(user_avg, by="userId") %>%
  mutate(pred = mu + b_i + b_u)%>%
  pull(pred)

movie_user_effect_rmse <- RMSE(test_set$rating, movie_user_effect_pred)
movie_user_effect_rmse
```

We call it "Movie User Effect Model".

```{r echo=FALSE}

rmse_results <- bind_rows(rmse_results,
                          tibble(method="3. Movie User Effects Model",
                                 RMSE =movie_user_effect_rmse))
rmse_results%>% knitr::kable()
```

### 4 Genres and release year effects

In our exploration, we found that genres and release year also seem to affect the prediction. Does considering these two factors improve the RMSE? We assume genres effect and release year effect. This can be represented as a following linear regression model. $g_{u,i}$ is genres effect, and $yr_{u,i}$ is release year effect.

$$
Y_{u, i} = \mu + b_i + b_u + \epsilon_{u, i} +g_{u, i} +yr_{u, i}
$$ The RMSE is produced by this code.

```{r echo=TRUE}

g_avg <- train_set %>%
  left_join(movie_avg, by= "movieId")%>%
  left_join(user_avg, by="userId") %>%
  group_by(genres) %>%
  summarize(g = mean(rating- mu -b_i -b_u))

y_avg <- train_set %>%
  left_join(movie_avg, by= "movieId")%>%
  left_join(user_avg, by="userId") %>%
  left_join(g_avg, by="genres") %>%
  group_by(release_year) %>%
  summarize(y = mean(rating- mu -b_i -b_u -g))

movie_user_genre_year_effect_pred <- test_set %>%
  left_join(movie_avg, by="movieId") %>%
  left_join(user_avg, by="userId") %>%
  left_join(g_avg, by="genres") %>%
  left_join(y_avg, by="release_year")%>%
  mutate(pred = mu + b_i + b_u + g + y)%>%
  pull(pred)

movie_user_genre_year_effect_rmse <- RMSE(test_set$rating,
                                          movie_user_genre_year_effect_pred)
movie_user_genre_year_effect_rmse
```

It has been much improved. We name it "Movie User Genre Release Year Effect Model".

```{r echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method=
                                  "4. Movie User Genre Release Year Effects Model",
                                 RMSE =movie_user_genre_year_effect_rmse))
rmse_results %>% knitr::kable()
```

### 5 Regularization (movie, user)

As we mentioned in the introduction, the challenge of a recommendation system is that the dataset is sparse and uneven. Namely, some users rate movies more than others, some movies are rated more. And some movies are rated high by very few users. That produces large errors, in consequence increases the RMSE. To improve this, regularization should be introduced. It penalizes large estimates which come from small sample sizes, and constrains the total variability.

We introduce a tuning parameter $\lambda$.

$$
\frac{1}{N}\sum_{u, i}(y_{u, i}-\mu-b_i)^2 + \lambda\sum_i b_{i}^2
$$The first term is a mean squared error. The second term is a penalty term. If sum of $b_i$ becomes larger, the penalty term increases the equation's value.

The value of $b$ that minimizes the equation is

$$
\hat{b}_{i}(\lambda) = \frac{1}{\lambda+n_i}\sum_{u=1}^{n_i}(Y_{u, i} - \hat{\mu}),
$$

$n$ is a number of ratings $b$ for movie $i$. $b$ can be expressed as a function of $\lambda$. Therefore, we can create a linear regression model using a parameter $\lambda$.

Based on this idea, we can incorporate the movie effect as follows.

$$
\sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
$$

First, we look for he value of $\lambda$ that minimize the RMSE. using a following code.

```{r echo=TRUE}
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){

  mu <- mean(train_set$rating)

  reg_movie_avg <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))

  reg_user_avg <- train_set %>%
    left_join(reg_movie_avg, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))

  predicted_ratings <- test_set %>%
    left_join(reg_movie_avg, by = "movieId") %>%
    left_join(reg_user_avg ,by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)

  return(RMSE(predicted_ratings, test_set$rating))

})
```

Then we plot $\lambda$ and RMSEs that the code produced..

```{r echo=FALSE}
qplot(lambdas,rmses)
```

The value of $\lambda$ which minimize the RMSEs is

```{r echo=FALSE}
lambda <- lambdas[which.min(rmses)]
lambda
```

The minimized value of the RMSE is

```{r echo=FALSE}
reg_movie_user_rmse <- rmses
reg_movie_user_rmse[5]
#[1] 0.8644477
```

We name it "Reg Movie User Effects Model".

```{r echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method="5. Reg Movie User Effects Model",  
                                 RMSE =reg_movie_user_rmse[5]))
rmse_results %>% knitr::kable()
```

Compared to the previous "Movie User Effect Model" ,the RMSE is improved, but it is worse than " Movie User Genre Release Year Effect Model". Thus we try to create another model.

### 6 Regularization (movie, user, genres, release_year)

In this model, we will take into account both genres and release year in the regularization. To find the value of $\lambda$ that minimize the RMSE, we use a following code.

```{r echo=TRUE}
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){

  mu <- mean(train_set$rating)

  reg_movie_avg <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))

  reg_user_avg <- train_set %>%
    left_join(reg_movie_avg, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))

  g_avg <- train_set %>%
    left_join(reg_movie_avg, by="movieId") %>%
    left_join(reg_user_avg, by="userId") %>%
    group_by(genres) %>%
    summarize(g = sum(rating - b_i -b_u - mu)/(n()+l))  

    y_avg <- train_set %>% 
    left_join(reg_movie_avg, by="movieId") %>%
    left_join(reg_user_avg, by="userId") %>%
    left_join(g_avg, by="genres") %>%
    group_by(release_year) %>%
    summarize(y = sum(rating - b_i -b_u -g - mu)/(n()+l)) 

  predicted_ratings <-test_set %>%
    left_join(reg_movie_avg, by = "movieId") %>%
    left_join(reg_user_avg, by = "userId") %>%
    left_join(g_avg, by="genres") %>%
    left_join(y_avg, by="release_year")%>%
    mutate(pred = mu + b_i + b_u + g + y) %>%
    pull(pred)

  return(RMSE(predicted_ratings, test_set$rating))
})
```

The value of $\lambda$ which minimize RMSE is

```{r echo=FALSE}
lambda <- lambdas[which.min(rmses)]
lambda
```

The RMSE is

```{r echo=FALSE}
min(rmses)
```

We call it "Reg Movie User Genre Release Year Effects Model".

```{r echo=FALSE}
rmse_results <- bind_rows(rmse_results, 
                          tibble(method=
                              "6. Reg Movie User Genre Release Year Effects Model",
                                RMSE =rmses[4.5]))
rmse_results %>% knitr::kable()
```

From this table, the final model "Reg Movie User Genre Release Year Effects Model" is proved to extract the least RMSE Value in the test set.

## Final Evaluation

Before the final evaluation, the validation set needs to be arranged to fit the train set. The Release Year column should be added from title column.

```{r include=FALSE}
validation <- validation %>% mutate(release_year = 
                                      as.numeric(str_sub(title,-5,-2)))
```

Find the value of $\lambda$ that minimize the RMSE. During the process, will will replace NAs in the validation with mu (the average rating in the training).

```{r include=FALSE}
lambdas <- seq(0, 10, 0.25)

#calculate rmse using "Reg Movie User Genre Release Year Effects Model" 

val_rmse <- sapply(lambdas, function(l){

  mu <- mean(train_set$rating)

  reg_movie_avg <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))

  reg_user_avg <- train_set %>%
    left_join(reg_movie_avg, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))

  g_avg <- train_set %>%
    left_join(reg_movie_avg, by="movieId") %>%
    left_join(reg_user_avg, by="userId") %>%
    group_by(genres) %>%
    summarize(g = sum(rating - b_i -b_u - mu)/(n()+l))  

  y_avg <- train_set %>%
    left_join(reg_movie_avg, by="movieId") %>%
    left_join(reg_user_avg, by="userId") %>%
    left_join(g_avg, by="genres") %>%
    group_by(release_year) %>%
    summarize(y = sum(rating - b_i -b_u -g - mu)/(n()+l)) 

  val_pred <-validation %>%
    left_join(reg_movie_avg, by = "movieId") %>%
    left_join(reg_user_avg, by = "userId") %>%
    left_join(g_avg, by="genres") %>%
    left_join(y_avg, by="release_year")%>%
    mutate(pred = mu + b_i + b_u + g + y) %>%
    pull(pred)

    #replace NA with mu(the average rating in the training)
    val_pred <-replace(val_pred, is.na(val_pred),mu)

    return(RMSE(val_pred,validation$rating))
})
```

Then we search lambda which minimize RMSE.

```{r echo=FALSE}
lambda <- lambdas[which.min(val_rmse)]
lambda
```

The final RMSE is;

```{r echo=FALSE}
Final_RMSE <-min(val_rmse)
Final_RMSE 
```

The model, "Reg Movie User Genre Release Year Effects Model" in which a regularized linear regression with predictors (userId, movieId, genres, and release year) produces the RMSE 0.8646978.

## Conclusion

In this report, we made six models. Our method was a linear regression. In the test set, the simple regression model "Movie User Genre Release Year Effects Model" showed improvement by 18.483% compared to the first baseline model ( only the average rating is considered) in terms of the RMSE.

Regularization has prove to be effective. However, if you take only two predictors (movie and user), our exploration showed that it was less effective than the 4th model. The regularized model was improved once it took more factors, genres and release year into the regularized model. The "Reg Movie User Genre Release Year Effects Model" proved to the best in the test set. Compared to the first model, it illustrated 18.499% improvement. Our final RMSE result from the validation set is 0.86469.

Having said that, this exploration revealed its limitation. As we increased predictors, the improvement became more and more slight. The difference between 4th model and 6th model in terms of the RMSE was approximately 0.00017. To improve the result, it is necessary to consider other factors like correlations between genres and introduce other method such as matrix factorization.
